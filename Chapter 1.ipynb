{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824e28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码1.1 Tensorflow中基于张量的矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27241d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8566343c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 21:41:08.449761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-02-21 21:41:08.530404: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: /usr/local/cuda-11.5/lib64/libcudnn.so.8: file too short; LD_LIBRARY_PATH: :/usr/local/cuda-11.5/lib64\n",
      "2022-02-21 21:41:08.530428: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-02-21 21:41:08.531425: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1,2],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4fbd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [1, 2]], dtype=int32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b3df73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.constant([[1],[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b103b03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
       "array([[1],\n",
       "       [2]], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ecf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967519b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
       "array([[5],\n",
       "       [5]], dtype=int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dab27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b061413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码1.2 Tensorflow和Tensorlayer中的梯度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b372d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a615a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, optimizer):\n",
    "    for x, y in dataset:\n",
    "        with tf.GradientTape as tape:\n",
    "            prediction = model(x)\n",
    "            loss = loss_fc(prediction, y)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b089f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c49ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码1.3 静态模型样例：多层感知器(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d2bd9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorlayer.layers import Input, Dense\n",
    "from tensorlayer.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "688e12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_model(inputs_shape):\n",
    "    ni = Input(inputs_shape)\n",
    "    nn = Dense(n_units=800, act=tf.nn.relu)(ni)\n",
    "    nn = Dense(n_units=800, act=tf.nn.relu)(nn)\n",
    "    nn = Dense(n_units=10, act=tf.nn.relu)(nn)\n",
    "    \n",
    "    M = Model(inputs=ni, outputs=nn)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42218da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Input  _inputlayer_1: [None, 784]\n",
      "[TL] Dense  dense_1: 800 relu\n",
      "[TL] Dense  dense_2: 800 relu\n",
      "[TL] Dense  dense_3: 10 relu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-92c5c787a041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mlp_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "MLP = get_mlp_model([None, 784])\n",
    "MLP.eval()\n",
    "outputs = MLP(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a7fe65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码1.4 动态模型样例：多层感知器(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df5c563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorlayer.layers import Input, Dense\n",
    "from tensorlayer.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b014fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MLPModel, self).__init__()\n",
    "        \n",
    "        self.dense1 = Dense(n_units=800, act=tf.nn.relu, in_channels=784)\n",
    "        self.dense2 = Dense(n_units=800, act=tf.nn.relu, in_channels=800)\n",
    "        self.dense3 = Dense(n_units=10, act=tf.nn.relu, in_channels=800)\n",
    "        \n",
    "    def forward(self, x, foo=False):\n",
    "        z = self.dense1(x)\n",
    "        z = self.dense2(z)\n",
    "        out = self.dense3(z)\n",
    "        \n",
    "        if foo:\n",
    "            out = tf.nn.softmax(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9deee104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Dense  dense_4: 800 relu\n",
      "[TL] Dense  dense_5: 800 relu\n",
      "[TL] Dense  dense_6: 10 relu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b986db510081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutputs_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moutputs_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "MLP = MLPModel()\n",
    "MLP.eval()\n",
    "\n",
    "outputs_1 = MLP(data, foo=True)\n",
    "outputs_2 = MLP(data, foo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecada07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35f4fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2467a38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Input  inut: [8, 3]\n",
      "[TL] Lambda  lambda_1: func: <function <lambda> at 0x7ff4cdab4290>, len_weights: 0\n",
      "[TL] Lambda  lambda_2: func: <function customize_fn at 0x7ff4cdab4cb0>, len_weights: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorlayer as tl\n",
    "\n",
    "x = tl.layers.Input([8, 3],name=\"inut\")\n",
    "y = tl.layers.Lambda(lambda x: 2*x)(x)\n",
    "def customize_fn(input, foo):\n",
    "    return foo*input\n",
    "z = tl.layers.Lambda(customize_fn, fn_args={'foo': 42})(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "200c18ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Input  input: [8, 3]\n",
      "[TL] Lambda  lambda_3: func: <function customize_fn at 0x7ff4cdad60e0>, len_weights: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "a = tf.Variable(1.0)\n",
    "def customize_fn(x):\n",
    "    return x + a\n",
    "x = tl.layers.Input([8, 3], name=\"input\")\n",
    "y = tl.layers.Lambda(customize_fn, fn_weights=[a])(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02da654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import numpy as np\n",
    "\n",
    "layers = [\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(5, activation=tf.nn.sigmoid),\n",
    "    tf.keras.layers.Dense(1, activation=tf.identity)\n",
    "]\n",
    "perception = tf.keras.Sequential(layers)\n",
    "_ = perception(np.random.random([10, 5]).astype(np.float32))\n",
    "\n",
    "class CustomizeModel(tl.models.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomizeModel, self).__init__()\n",
    "        self.dense = tl.layers.Dense(in_channels=1, n_units=5)\n",
    "        self.lambdalayer = tf.layers.Lambda(perception, perception.trainable_variables)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.dense(x)\n",
    "        z = self.lambdalayer(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2417c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cb70fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多层感知器：MNIST数据集上的图像分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5750166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Load or Download MNIST > data/mnist\n",
      "[TL] Downloading train-images-idx3-ubyte.gz...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1211 of 1211) |####################| Elapsed Time: 0:03:41 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Succesfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "[TL] data/mnist/train-images-idx3-ubyte.gz\n",
      "[TL] Downloading train-labels-idx1-ubyte.gz...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (4 of 4) |##########################| Elapsed Time: 0:00:00 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Succesfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "[TL] Downloading t10k-images-idx3-ubyte.gz...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (202 of 202) |######################| Elapsed Time: 0:00:22 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Succesfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "[TL] data/mnist/t10k-images-idx3-ubyte.gz\n",
      "[TL] Downloading t10k-labels-idx1-ubyte.gz...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1 of 1) |##########################| Elapsed Time: 0:00:00 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Succesfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "[TL] Input  _inputlayer_2: [None, 784]\n",
      "[TL] Dropout dropout_1: keep: 0.800000 \n",
      "[TL] Dense  dense_7: 800 relu\n",
      "[TL] Dropout dropout_2: keep: 0.500000 \n",
      "[TL] Dense  dense_8: 800 relu\n",
      "[TL] Dropout dropout_3: keep: 0.500000 \n",
      "[TL] Dense  dense_9: 10 No Activation\n",
      "[TL] Finished! use `tensorboard --logdir=None/` to start tensorboard\n",
      "[TL] Start training the network ...\n",
      "[TL] Epoch 1 of 20 took 8.149116s\n",
      "[TL]    train loss: 0.422891\n",
      "[TL]    train acc: 0.878626\n",
      "[TL] Epoch 5 of 20 took 3.685423s\n",
      "[TL]    train loss: 0.194406\n",
      "[TL]    train acc: 0.941526\n",
      "[TL] Epoch 10 of 20 took 4.282970s\n",
      "[TL]    train loss: 0.123684\n",
      "[TL]    train acc: 0.962740\n",
      "[TL] Epoch 15 of 20 took 4.393666s\n",
      "[TL]    train loss: 0.089894\n",
      "[TL]    train acc: 0.973357\n",
      "[TL] Epoch 20 of 20 took 4.597843s\n",
      "[TL]    train loss: 0.068480\n",
      "[TL]    train acc: 0.979507\n",
      "[TL] Total training time: 102.645401s\n",
      "[TL] Start testing the network ...\n",
      "[TL] [*] Saving TL weights into model.h5\n",
      "[TL] [*] Saved\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=[-1, 784])\n",
    "\n",
    "ni = tl.layers.Input([None, 784])\n",
    "nn = tl.layers.Dropout(keep=0.8)(ni)\n",
    "nn = tl.layers.Dense(n_units=800, act=tf.nn.relu)(nn)\n",
    "nn = tl.layers.Dropout(keep=0.5)(nn)\n",
    "nn = tl.layers.Dense(n_units=800, act=tf.nn.relu)(nn)\n",
    "nn = tl.layers.Dropout(keep=0.5)(nn)\n",
    "nn = tl.layers.Dense(n_units=10, act=None)(nn)\n",
    "\n",
    "network = tl.models.Model(inputs=ni, outputs=nn, name=\"mlp\")\n",
    "\n",
    "def acc(_logits, y_batch):\n",
    "    return tf.reduce_mean(\n",
    "        tf.cast(\n",
    "            tf.equal(\n",
    "                tf.argmax(_logits, 1),\n",
    "                tf.convert_to_tensor(y_batch, tf.int64)),\n",
    "        tf.float32),\n",
    "        name='accuracy'\n",
    "    )\n",
    "\n",
    "tl.utils.fit(\n",
    "    network, \n",
    "    train_op=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "    cost=tl.cost.cross_entropy,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    acc=acc,\n",
    "    batch_size=256,\n",
    "    n_epoch=20,\n",
    "    X_val=X_val, y_val=y_val, eval_train=True,\n",
    ")\n",
    "\n",
    "tl.utils.test(\n",
    "    network,\n",
    "    acc=acc,\n",
    "    X_test=X_test, y_test=y_test,\n",
    "    batch_size=None,\n",
    "    cost=tl.cost.cross_entropy,\n",
    ")\n",
    "\n",
    "network.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea6415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1919f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积神经网络：CIFAR-10数据集上的图像分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e275a40",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorlayer' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-438d9d99a5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_fn_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorlayer' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers import Input, Dense, Conv2d, BatchNorm2d, MaxPool2d, Flatten\n",
    "from tensorlayer.models import Model\n",
    "import numpy as np\n",
    "\n",
    "def _fn_train(img, target):\n",
    "    img = tl.prepro.crop(img, 24, 24, False)\n",
    "    img = tl.prepro.flip_axis(img, is_random=True)\n",
    "    img = tl.samplewise_norm(img)\n",
    "    target = np.reshape(target, ())\n",
    "    return img, target\n",
    "\n",
    "train_ds = tl.data.CIFAR10(train_or_test='train', shape=(-1, 32, 32, 3))\n",
    "train_dl = tl.data.Dataloader(train_ds, transforms=[_fn_train], shuffle=True, batch_size=batch_size, output_types=(np.float32, np.int32))\n",
    "\n",
    "test_ds = tl.data.CIFAR10(train_or_test='test', shape=(-1, 32, 32, 3))\n",
    "test_dl = tl.data.Dataloader(test_ds, batch_size=batch_size)\n",
    "\n",
    "for X_batch, y_batch in train_dl:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31f8b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_batchnorm(inputs_shape):\n",
    "    W_init = tl.initializers.truncated_normal(stddev=5e-2)\n",
    "    W_init2 = tl.initializers.truncated_normal(stddev=0.04)\n",
    "    b_init2 = tl.initializers.constant(value=0.1)\n",
    "    \n",
    "    ni = Input(inputs_shape)\n",
    "    \n",
    "    nn = Conv2d(64, (5, 5), (1, 1), padding=\"SAME\", W_init=W_init, b_init=None)(ni)\n",
    "    nn = BatchNorm2d(decay=0.99, act=tf.nn.relu)(nn)\n",
    "    nn = MaxPool2d((3, 3), (2, 2), padding=\"SAME\")(nn)\n",
    "    \n",
    "    nn = Conv2d(64, (5, 5), (1, 1), padding=\"SAME\", W_init=W_init, b_init=None)(ni)\n",
    "    nn = BatchNorm2d(decay=0.99, act=tf.nn.relu)(nn)\n",
    "    nn = MaxPool2d((3, 3), (2, 2), padding=\"SAME\")(nn)\n",
    "    \n",
    "    nn = Flatten(nn)\n",
    "    nn = Dense(n_units=384, act=tf.nn.relu, W_init=W_init2, b_init=b_init2)(nn)\n",
    "    nn = Dense(n_units=193, act=tf.nn.relu, W_init=W_init2, b_init=b_init2)(nn)\n",
    "    nn = Dense(n_units=10, act=None, W_init=W_init2)(nn)\n",
    "    \n",
    "    M = Model(inputs=ni, outputs=nn, name=\"cnn\")\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5ebf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bc3c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列到序列模型：聊天机器人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "126967b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_seq_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-acc8148741b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model = Seq2seq(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdecoder_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcell_enc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRUCell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcell_dec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRUCell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_seq_length' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.models import Seq2seq\n",
    "\n",
    "model = Seq2seq(\n",
    "    decoder_seq_length=decoder_seq_length,\n",
    "    cell_enc=tf.keras.layers.GRUCell,\n",
    "    cell_dec=tf.keras.layers.GRUCell,\n",
    "    n_layer=3,\n",
    "    n_units=256,\n",
    "    embedding_layer=tl.layers.Embedding(vocabulary_size=vocabulary_size, embedding_size=emb_dim),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
